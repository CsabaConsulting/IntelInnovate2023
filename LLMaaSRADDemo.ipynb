{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7e56401-7142-4d77-b4fb-3b47cae5c56c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -qU gradio\n",
    "!pip install pinecone-client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1edfa2-385c-4c6e-95cd-a52e2db9952b",
   "metadata": {},
   "source": [
    "You may need to restart the kernel here if packages were installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7acdcc5f-ffbe-43d5-9661-d86405dcbd76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\") or \"OPENAI_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27b8f901-bb49-47ac-9c41-8c450e29f678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import pinecone\n",
    "\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\") or \"PINECONE_API_KEY\"\n",
    "# find your environment next to the api key in pinecone console\n",
    "env = os.getenv(\"PINECONE_ENVIRONMENT\") or \"gcp-starter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8999843c-6485-4567-9cb6-230d6b4e13b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhoAmIResponse(username=None, user_label=None, projectname='14d2815')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone.init(api_key=api_key, environment=env)\n",
    "pinecone.whoami()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1012a3ae-993c-4376-86b4-574620d78c7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.00464,\n",
       " 'namespaces': {'': {'vector_count': 464}},\n",
       " 'total_vector_count': 464}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_name = 'cnvrg-openai'\n",
    "# connect to index\n",
    "index = pinecone.GRPCIndex(index_name)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a9f16b6-1fad-4671-bca0-88931317fa63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://cismography.medium.com/how-to-integrate-custom-llm-using-langchain-a-gpt4all-example-cfcb6d26fc3\n",
    "from pydantic import Field\n",
    "from typing import List, Mapping, Optional, Any\n",
    "from langchain.llms.base import LLM\n",
    "import requests\n",
    "\n",
    "class LLMaaS(LLM):\n",
    "    \"\"\"\n",
    "    A custom LLM class that integrates LLMaaS models\n",
    "    \n",
    "    Arguments:\n",
    "\n",
    "    model_folder_path: (str) Folder path where the model lies\n",
    "    model_name: (str) The name of the model to use (<model name>.bin)\n",
    "    allow_download: (bool) whether to download the model or not\n",
    "\n",
    "    temperature: (str) Temperature to use for sampling\n",
    "    top_p: (float) The top-p value to use for sampling\n",
    "    top_k: (float) The top k values use for sampling\n",
    "    max_new_tokens: (str) The maximum numbers of tokens to generate\n",
    "    repetition_penalty: (float) The penalty to apply repeated tokens\n",
    "    \n",
    "    \"\"\"\n",
    "    model_folder_path: str = Field(None, alias='model_folder_path')\n",
    "    model_name: str = Field(None, alias='model_name')\n",
    "    allow_download: bool = Field(None, alias='allow_download')\n",
    "\n",
    "    # # all the optional arguments\n",
    "\n",
    "    temperature:        Optional[float] = 0.5\n",
    "    top_p:              Optional[float] = 1\n",
    "    top_k:              Optional[int]   = 50\n",
    "    max_new_tokens:     Optional[int]   = 250\n",
    "    repetition_penalty: Optional[float] = 1\n",
    "\n",
    "    def __init__(self, model_name, allow_download=False, **kwargs):\n",
    "        super(LLMaaS, self).__init__()\n",
    "        self.model_name = model_name\n",
    "        self.allow_download = allow_download\n",
    "        self.temperature = kwargs.get(\"temperature\", 0.5)\n",
    "        self.top_p = kwargs.get(\"top_p\", 1)\n",
    "        self.top_k = kwargs.get(\"top_k\", 50)\n",
    "        self.max_new_tokens = kwargs.get(\"max_new_tokens\", 250)\n",
    "        self.repetition_penalty = kwargs.get(\"repetition_penalty\", 1)\n",
    "        \n",
    "    def auto_download(self) -> None:\n",
    "        \"\"\"\n",
    "        This method will download the model to the specified path\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def _get_model_default_parameters(self):\n",
    "        return {\n",
    "            \"max_new_tokens\": self.max_new_tokens,\n",
    "            \"top_k\": self.top_k,\n",
    "            \"top_p\": self.top_p,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"repetition_penalty\": self.repetition_penalty,\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"\n",
    "        Get all the identifying parameters\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'model_name' : self.model_name,\n",
    "            'model_parameters': self._get_model_default_parameters\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return 'mpt'\n",
    "    \n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None, **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            prompt: The prompt to pass into the model.\n",
    "            stop: A list of strings to stop generation when encountered\n",
    "\n",
    "        Returns:\n",
    "            The string generated by the model        \n",
    "        \"\"\"\n",
    "        headers = {\n",
    "            'Content-Type': \"application/json\",\n",
    "            'x-api-key': \"{your LLMaaS token here}\"\n",
    "        }\n",
    "        url = \"https://app.llm.cnvrg.io/api/v1/chat\"\n",
    "\n",
    "        temperature = kwargs.get(\"temperature\", self.temperature)\n",
    "        top_p = kwargs.get(\"top_p\", self.top_p)\n",
    "        top_k = kwargs.get(\"top_k\", self.top_k)\n",
    "        max_new_tokens = kwargs.get(\"max_new_tokens\", self.max_new_tokens)\n",
    "        repetition_penalty = kwargs.get(\"repetition_penalty\", self.repetition_penalty)\n",
    "\n",
    "        kwargs.get(\"data\", {})\n",
    "        payload = {\n",
    "            \"model_uuid\": \"7e0a4264-4026-4ebb-8eb0-2cdedcace07b\",\n",
    "            \"data\": {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"text\": prompt,\n",
    "                    \"user_role\": True\n",
    "                }\n",
    "            ],\n",
    "            \"inference_params\": {\n",
    "                \"max_new_tokens\": max_new_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"repetition_penalty\": repetition_penalty,\n",
    "                \"top_p\": top_p,\n",
    "                \"do_sample\": \"True\",\n",
    "                \"num_beams\": 0,\n",
    "                \"top_k\": top_k\n",
    "            },\n",
    "            \"system_prompt\": \"- You are a helpful assistant chatbot trained by Intel.\\n- You answer questions.\\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes\"}\n",
    "        }\n",
    "\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f16483d-ef9d-4a7d-9ff4-2d15f3d16429",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0929569f-414c-48fa-903d-f4dd3577e93f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This will allow to query a response without having to load files repeatedly.\n",
    "def data_querying(input_text):\n",
    "    # We Must reinitialize Pinecone in oder to load our previously created index.\n",
    "    api_key = os.getenv(\"PINECONE_API_KEY\") or \"PINECONE_API_KEY\"\n",
    "    # find your environment next to the api key in pinecone console\n",
    "    environment = os.getenv(\"PINECONE_ENVIRONMENT\") or \"gcp-starter\"\n",
    "\n",
    "    os.environ['PINECONE_API_KEY'] = api_key\n",
    "\n",
    "    index_name = \"cnvrg-openai\"\n",
    "    pinecone.init(api_key=api_key, environment=environment)\n",
    "\n",
    "    embed_model = 'text-embedding-ada-002'\n",
    "\n",
    "    embed = OpenAIEmbeddings(\n",
    "      model=embed_model,\n",
    "      openai_api_key=openai.api_key\n",
    "    )\n",
    "\n",
    "    # load pinecone index for langchain\n",
    "    index = pinecone.Index(index_name)\n",
    "\n",
    "    # This text field represents the field that the text contents of your document are stored in\n",
    "    text_field = \"question\"\n",
    "\n",
    "    # load pinecone index for langchain\n",
    "    index = pinecone.Index(index_name)\n",
    "\n",
    "    vectorstore = Pinecone(\n",
    "      index, embed.embed_query, text_field\n",
    "    )\n",
    "    # Query the vectorized data\n",
    "    vectorstore.similarity_search(\n",
    "      input_text,  # our search query\n",
    "      k = 2  # return k most relevant docs\n",
    "    )\n",
    "\n",
    "    # vectorstore = Pinecone.from_existing_index(\n",
    "    #     index_name,\n",
    "    #     embedding=embed, \n",
    "    #     namespace=\"SessionIndex\"\n",
    "    # )\n",
    "\n",
    "    # Using LangChain we pass in our model for text generation.\n",
    "    llm = LLMaaS(temperature=0.5, model_name=\"mpt7b_2bs_5epoch\")\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vectorstore.as_retriever()\n",
    "    )\n",
    "    # Finally we return the result of the search query. \n",
    "    response = qa.run(input_text)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b19012-39b1-4369-b5c1-eedadc808218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "#Create your gradio Interface\n",
    "iface = gr.Interface(fn=data_querying,\n",
    "                     inputs=gr.inputs.Textbox(lines=7, label=\"Enter your question\"),\n",
    "                     outputs=\"text\",\n",
    "                     title=\"Test LLMaaS RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf1dac7e-a3c7-4b28-b1ed-742a72a3b625",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://06578dde3b19c9beb1.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://06578dde3b19c9beb1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csaba/anaconda3/lib/python3.9/site-packages/langchain/vectorstores/pinecone.py:59: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Launches Gradio App  \n",
    "iface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
